{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbT6glhQLtmF",
        "outputId": "ece6031e-19ed-4255-971b-2c83aec95151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-hy9d2mno/unsloth_d97bba9e14074f8cb300be0f900843bf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-hy9d2mno/unsloth_d97bba9e14074f8cb300be0f900843bf\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit 229e2ecc67756f36316dfcbea42396f59eef44e0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: unsloth_zoo>=2025.9.9 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.9.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.0)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.9.32)\n",
            "Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.55.4,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.55.4)\n",
            "Requirement already satisfied: datasets<4.0.0,>=3.4.1 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.29.5)\n",
            "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.35.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.9)\n",
            "Requirement already satisfied: bitsandbytes>=0.45.5 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.47.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.1.10)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.55.4,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.55.4,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.55.4,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.2)\n",
            "Requirement already satisfied: torchao in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.9.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.10.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.9.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.0)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.9.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.10.1)\n",
            "Requirement already satisfied: trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.9.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.6)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.9.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.17.1)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.9.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.9.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.9.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.2)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.12.15)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.11.1.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.2)\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.12/dist-packages (0.0.32.post2)\n",
            "Requirement already satisfied: trl<0.9.0 in /usr/local/lib/python3.12/dist-packages (0.8.6)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
            "Requirement already satisfied: google-cloud-translate in /usr/local/lib/python3.12/dist-packages (3.21.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-translate) (2.38.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=1.4.4 in /usr/local/lib/python3.12/dist-packages (from google-cloud-translate) (2.4.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-translate) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-translate) (5.29.5)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-translate) (0.14.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (1.75.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (4.15.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "\n",
        "!pip install sacrebleu google-cloud-translate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install sacrebleu google-cloud-translate\n",
        "\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506,
          "referenced_widgets": [
            "1496a3f4417b497598c75c64da22191a",
            "f1ef82794c8e432da88c98f0bb746fb4",
            "2032fd154c534aa0a7dadcbcbff84569",
            "872d49be37b24754a40d03229f59e13a",
            "cc42b714ce2c4e438ed9259aff4958ec",
            "1d86917569e341ae96cff555b2de220e",
            "bc100ae61c6d4f278b7439b34c7b51a4",
            "7e97cdd29fe74bcf8bd99ab16be1038f",
            "f13c7a5c32594cc9a76f4be2aec2db7c",
            "1e67b965a7594b2886f0a1afc3c232c6",
            "134ae71ce1c8451ea683ad7ccf4794e2",
            "bb10422a672743d586a2dc5b1c0abd2a",
            "219a499af2c94092965c433033d71a20",
            "0b035d87bdb046b5843916013c38b930",
            "866462bd29da40668d1b8eedf3d7c0eb",
            "134578a656c849a4b0de3e81dbc95e2d",
            "e75554b2464e4bd8b7cf8a40ab595776",
            "e1159c46ff044cb18e9c7d2399f04de5",
            "72c5e3b19e0643b99d9664ee3a13a147",
            "3b3bb3bd20d34a53ad342d379b34ceeb"
          ]
        },
        "id": "bSK4dj4pMiDN",
        "outputId": "c4fd3284-f084-4325-d005-479a304b7ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
            "Requirement already satisfied: google-cloud-translate in /usr/local/lib/python3.12/dist-packages (3.21.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-translate) (2.38.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=1.4.4 in /usr/local/lib/python3.12/dist-packages (from google-cloud-translate) (2.4.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-translate) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-translate) (5.29.5)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-translate) (0.14.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (1.75.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (4.15.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2025.8.3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1496a3f4417b497598c75c64da22191a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "SYNTHETIC_FILE = \"/content/drive/MyDrive/synthetic_data.csv\"\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = \"You are a highly skilled translator. Your task is to accurately translate English text to Tamil, preserving the original meaning and tone.\"\n",
        "\n",
        "def format_dataset(row):\n",
        "    \"\"\"\n",
        "    Formats each row of data into the Gemma chat template. Gemma does not support 'system' role,\n",
        "    so merge it into the 'user' message. Use 'model' role for assistant responses.\n",
        "    \"\"\"\n",
        "    english_text = row['english']\n",
        "    tamil_text = row['tamil']\n",
        "\n",
        "    user_content = f\"{SYSTEM_PROMPT}\\n\\n{english_text}\"\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": user_content},\n",
        "            {\"role\": \"model\", \"content\": tamil_text}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "print(\"--- Preparing Dataset ---\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(SYNTHETIC_FILE)\n",
        "    print(f\"Loaded {len(df)} rows from {SYNTHETIC_FILE}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Make sure you have uploaded '{SYNTHETIC_FILE}' to the Colab session.\")\n",
        "    raise\n",
        "\n",
        "# Clean data and apply the formatting\n",
        "df.dropna(inplace=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "formatted_data = df.apply(format_dataset, axis=1).tolist()\n",
        "\n",
        "# Split into training and evaluation sets\n",
        "train_data, eval_data = train_test_split(formatted_data, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "eval_dataset = Dataset.from_list(eval_data)\n",
        "\n",
        "print(f\"\\nCreated {len(train_dataset)} training examples and {len(eval_dataset)} evaluation examples.\")\n",
        "print(\"\\nVerifying the first training example:\")\n",
        "print(train_dataset[0]['messages'])\n",
        "print(\"\\n✅ Data preparation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwICj9DsM3Ix",
        "outputId": "211c0b46-1b50-4744-81b1-5b589e6db740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Preparing Dataset ---\n",
            "Loaded 2044 rows from /content/drive/MyDrive/synthetic_data.csv\n",
            "\n",
            "Created 1836 training examples and 204 evaluation examples.\n",
            "\n",
            "Verifying the first training example:\n",
            "[{'content': 'You are a highly skilled translator. Your task is to accurately translate English text to Tamil, preserving the original meaning and tone.\\n\\nMy grandmother makes the best sweets.', 'role': 'user'}, {'content': ' என் பாட்டி சிறந்த இனிப்புகளைச் செய்வார்.', 'role': 'model'}]\n",
            "\n",
            "✅ Data preparation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_NAME = \"unsloth/gemma-3-4b-it-bnb-4bit\"  # Gemma 3 4B instruct, quantized for efficiency\n",
        "MAX_SEQ_LENGTH = 8192\n",
        "\n",
        "# 1. Load the model using Unsloth's FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "tokenizer_inner = tokenizer.tokenizer\n",
        "\n",
        "\n",
        "def format_example(example):\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "print(\"--- Applying chat template to datasets ---\")\n",
        "train_dataset = train_dataset.map(format_example)\n",
        "eval_dataset = eval_dataset.map(format_example)\n",
        "print(\"\\nVerifying formatted text (first example):\")\n",
        "print(train_dataset[0]['text'])\n",
        "print(\"\\n✅ Datasets formatted.\")\n",
        "\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=True,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer_inner,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=2,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=42,\n",
        "        output_dir=\"outputs\",\n",
        "        save_strategy=\"epoch\",\n",
        "        eval_strategy=\"epoch\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "print(\"--- Starting Fine-Tuning ---\")\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n✅ Fine-tuning complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "08f140ed37264de49c6ed035feab9bf8",
            "0e594d46d61d4490b143397c10436cfe",
            "1a992c7343b24cb98de240b961fd2b87",
            "5c8cc9a4205e4c509832cbcffced7161",
            "7817093c164d400999c60682e085b391",
            "9aea5e46304e4d2292100399cb49add5",
            "7b80813e2e8e400e9a64f17d52132338",
            "00c22aa3670c40e688e20b1a5370312f",
            "9c2f4a9f4fc74d36868e12d1d6f9b9e4",
            "4cf382ee02564d25884c8fdfd7595455",
            "063521c6d97a485f8be84807a0eb22ef",
            "1990f64342a84d948533496ae86273a9",
            "7611535a1d684f649cadd61f77cb0528",
            "3ffefbfbc38f4d3a96cad8f4317d4865",
            "1f41ccc486d44cc8bad24627525d27b7",
            "edd400b0ea8b4a059cd941c09989a341",
            "9a66d8de9b0c4e2d81f5234eaee59a65",
            "2b4ceb35ccbd4a118feaa9c6f0939888",
            "15e5aed3797a43b19bd8a1ac65f706f8",
            "c1808f5dbe16460ea7afacf6f00eb583",
            "e9f9f1c014604ff88dcf02ed21598d1d",
            "bb5e9d4e558244c48dfb381ec7bd45cc",
            "b1ac7fdc5f1c4869803786833efdf775",
            "8122f0220f984d548d3d4ad58bcd79a0",
            "8788b3c7e3eb42e48ba2a24b6a3d01e6",
            "25f597448237443a8ad4f9964fc1207a",
            "24b7ddb214c94935b70af3cba71e39e1",
            "be358f7b05ab4be1887004e7750d0a25",
            "b9d4716fa23e46f098bab3fb772f4889",
            "ed17274850d14783ae941a6dd10e3bda",
            "001d4e81ab184be8bfbeceec57c85eb9",
            "b1d6742ea3654da285399d7c4dcd2f66",
            "d086838c19c9460dab882cda70e49f00",
            "513cea6484554f52820cfcaff71b29b4",
            "b2916e1de69743afa569755aef9175ac",
            "13718f34c666473c9704eb6781a78d3d",
            "f333d8330e0f45a5a4d6ae2a8c6e5408",
            "0f00db05deda49cdb7a0bb71de138e2c",
            "ef40022e7832425f85560927092e9eda",
            "63f0b6e44b3946f8ba73b085d8ab2817",
            "226a3c36c5d14996b885d7ad21bb5faf",
            "a8101e7d638e4daf8b21ca4b61eb1a84",
            "3f2462e4c0574734bf90d9d1c4d61a97",
            "e15a59ac540646e389340bf020df7731",
            "b470ba746cef49a08ca1fffdf47a04b5",
            "91b36e758ed04a4b88c75222801b37f0",
            "8693359787a643ada9b072f991797e3d",
            "9077d33fe9fd4c029814707aae47dea0",
            "fd4d2e4cb8db4a10a94aa6678b1670ad",
            "386a9b1fc1d3496ea5c0f0d36c183737",
            "f71474c2c71b46849450221090da069c",
            "46491c310a284b08aaef862d9bbb5dcc",
            "98f9aebecfca417c9f080779b9ae728c",
            "d39dd12815a54433928ed4162c895b71",
            "51358b0610284e20a715a5311606b892",
            "ca2fa240b05f4682828c9e8a3ea4ff75",
            "e65a62fd8dba4d779e97bb32365446e7",
            "ae639a0b2b9347c2ae1a345a2e7ea726",
            "391c44eca7ca4cea9959a98ebe02f48d",
            "bb66b6b7aa2d497e9dafa7f6cde7b6b4",
            "202d4e2fac3e4f61a480195732e07990",
            "0f2ca97f112446919ce7bc9db944b4b8",
            "64b9ceb81d78493c81b068b9536b1679",
            "b2b242b3390f400287bfbbd4e37912ca",
            "25d4b206a72347d6a4442dbc956ab064",
            "796f8b8acf76443d88e1768a0469de15",
            "f45a91b076f8423785db1a6253ced9f2",
            "7f8b8bf35200497fb6294c9aa099722b",
            "152c7245536247a0a93a383559049b1a",
            "eca05d8791894e8dba7ed209f9bd8f42",
            "b2da269621784008b4b566cf182324bc",
            "fa7e1543a0824a568a253552bb345091",
            "2b07dcf473ec4bb2b9967fbc6252981c",
            "ff6287cd32bb4cde858db4629a68ddcb",
            "9bfd9bf4053449f2952bf43f152a78e3",
            "73d4cd4652d34fdda8516c217cd05a1a",
            "5b30331fa29c44b2b36be62a29e46c41",
            "e2007ac766754a068b73b6f422da4409",
            "a5dc7d4298d243eeaf14e3cf5585d405",
            "34a7d0d855c44959a3c26eff9b07d5c3",
            "4875505edebf492dbdcbcc573fa55749",
            "2f19462cb02a4d9a89cca48d662fbace",
            "209370768dd74423bc42cb59ed51978a",
            "098cae2479294e44968497e27f8b65ae",
            "88d924a7ec0447ce80641524860019aa",
            "33e2fb618da54253b1ed46feb5f5c1ec",
            "d02985128f354b329e064b01e623f6e4",
            "0f4124b7c9414bf9b2b293145003674a",
            "6a22544f1cd74645bb3760fd933338c4",
            "50474d68e02b45cea2816458acb60671",
            "74d7f11fb90843ceba41709d0a0bf5bd",
            "c9db1a6ced6243e9b832ad7feae69ba8",
            "27428b48fdf749829750abdce36a39ec",
            "cec4247e4691413aa14a7576c0a1a796",
            "b4f9ee61077c4481a6352a3cc636203e",
            "9abd7fce70834076931108c204cb8d94",
            "1b09168208ae4527a931dfc3f29e72a4",
            "51b340e6ad714608b7a3bf3ca8a9f392",
            "68fcf849a51e4b0ab0824674d7548d67",
            "82b8daed1b74480cb6a04ec8586320d7",
            "50945faa9e6444b999a77e76141b9eac",
            "857f714448af4661b9b1a0ad0f14ed96",
            "bac949db56bf4b96b38eab39cb6ea929",
            "5fb72fda840e4144958aa04d6c9cbeba",
            "7ef2ed6a2b4e465694cb3a286746b8ab",
            "40e513a686484d4093f5577d04cb82c3",
            "a03d4214467944ecb6d16e337dd011ce",
            "1a0a70c8924844d280f3d2c09ab7cbab",
            "51c2d1cbfb7b4913a4da4c3c5781a1d9",
            "1ec16f7e430e4a989848d8d0d9b3ab04",
            "882ec5eb66ed4a5589f08e3ba8534a23",
            "88c7611e2e3f452c94214e4be00fa96d",
            "ccbb5dbd078b47fd9319e68181e76ef5",
            "82d7a210484c455eab86ec7f8f3914d7",
            "af12d3e7e27f4273a383990db344f706",
            "577c0be310d94de2b06753b5aa6f84f7",
            "d751f96ca6254001ac4ae0b11deffe20",
            "77e08906eb5543bb8cd302fcfba2b5af",
            "4974a113832d45828a43e8de84fc9ef5",
            "5fd3c115a4d349e691e1152c79abfc50",
            "86546f9561fc477c94b43b8eef766b7e",
            "e775a1d3a62b4b98a82eb7458f8d3ff4",
            "7dfebe6bf07c46bc8c498640a783ff87",
            "f6117d24df9a48f38115bf8174575ec2",
            "fae57b6c13ba4e109e05a07f1f81ebf3",
            "3f69a6b942d046ccba441146d6ec5d72",
            "d504154ba1fc49d8aa12ade3c67a1637",
            "f3a5ae6b9cb6427a91422020c50f096b",
            "aeb0afa62d5e465eb94521d1126077fa",
            "20f6d3ae8daf4cbc96f8ae96631ce7a4",
            "39507140d5cb4e39b3a481197d994136",
            "5d56b305a39e42fe84bd4255defd29ab",
            "95c73a1b3a474e0e9af4c18345a4467c",
            "d916f0fdb1e34981a81de892e99cbbe7",
            "142705c7c30c4072b33b0996f4c3b42c",
            "332ff9e93d26406d86e8952abefc8ca1",
            "1e788c6cc72447a4851902dfc0172649",
            "a4468cf111b0463ebd1b93c2578e9da4",
            "f935269d0c0e44cf81a6468d7b49e359",
            "40792ac9c3c24220a2a8bc8cdea7af54",
            "6826d8e9de1741fe95d1b42200d6042b",
            "905743facbc540f7b9f7660c5e27928f",
            "c859d824b2bc4ea197ac8877d4a7c0c5",
            "40f5b7c0d5134609a8c346984c5dfe21",
            "64ebce95ae1f4f2c904c64b2f7e0a201",
            "1ea76127f23749549d07fa04ce41336e",
            "e78be649423949cbac7d727f74184b74",
            "66d46d42e51a44128554e3337ae13db2",
            "f66ae444e69541648472ea79b65bac87",
            "7d31496084f1492e8a67039202a02468",
            "01aab557e5ec4a5c84bcbf9e37ded5aa",
            "7be47f1a36de4a24876b1a80bb1aaedd",
            "e4aebe550a4242c581bbd70fd53719c0",
            "7946891d465946dfa7a0010a67818715",
            "e03fccaf59c3453cbf1e5989c3636064",
            "5c7b6f501b864fbbb7711f7c65aadf2b",
            "b62787ac3cac4d52b616eb83dfbb6348",
            "a7b5e423f6c34b4cbf427987bdbf2602",
            "22148eec4de541f7872b2043d15a58a6",
            "851c41b0be2647d3ad324a6bcdbf103a",
            "1c6184b7ada543d5a0b32e7070c960a1",
            "9cdfad403d1e477fae9bbef1f6baccd1",
            "2ddbe650d4384176a188531e21b5aee0",
            "7fd2f9110ab742cc82c198f8a35cdf59",
            "c58dd808ac36473396052258b0727898"
          ]
        },
        "id": "ALV9u3bWNPMQ",
        "outputId": "0d168e83-471a-4d60-d623-11e2e2ccd48d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.9.7: Fast Gemma3 patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.23G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08f140ed37264de49c6ed035feab9bf8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1990f64342a84d948533496ae86273a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1ac7fdc5f1c4869803786833efdf775"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "513cea6484554f52820cfcaff71b29b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b470ba746cef49a08ca1fffdf47a04b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca2fa240b05f4682828c9e8a3ea4ff75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f45a91b076f8423785db1a6253ced9f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2007ac766754a068b73b6f422da4409"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a22544f1cd74645bb3760fd933338c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82b8daed1b74480cb6a04ec8586320d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "882ec5eb66ed4a5589f08e3ba8534a23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Applying chat template to datasets ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1836 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e775a1d3a62b4b98a82eb7458f8d3ff4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/204 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95c73a1b3a474e0e9af4c18345a4467c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Verifying formatted text (first example):\n",
            "<bos><start_of_turn>user\n",
            "You are a highly skilled translator. Your task is to accurately translate English text to Tamil, preserving the original meaning and tone.\n",
            "\n",
            "My grandmother makes the best sweets.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "என் பாட்டி சிறந்த இனிப்புகளைச் செய்வார்.<end_of_turn>\n",
            "\n",
            "\n",
            "✅ Datasets formatted.\n",
            "Unsloth: Making `base_model.model.model.vision_tower.vision_model` require gradients\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/1836 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40f5b7c0d5134609a8c346984c5dfe21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/204 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e03fccaf59c3453cbf1e5989c3636064"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,836 | Num Epochs = 2 | Total steps = 460\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 32,788,480 of 4,332,867,952 (0.76% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Fine-Tuning ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marsathsafnaajeeba\u001b[0m (\u001b[33marsathsafnaajeeba-ardev\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250924_055010-anobnyss</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/arsathsafnaajeeba-ardev/huggingface/runs/anobnyss' target=\"_blank\">morning-plasma-3</a></strong> to <a href='https://wandb.ai/arsathsafnaajeeba-ardev/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/arsathsafnaajeeba-ardev/huggingface' target=\"_blank\">https://wandb.ai/arsathsafnaajeeba-ardev/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/arsathsafnaajeeba-ardev/huggingface/runs/anobnyss' target=\"_blank\">https://wandb.ai/arsathsafnaajeeba-ardev/huggingface/runs/anobnyss</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [openai] in use.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [460/460 22:21, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.410800</td>\n",
              "      <td>0.563631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.499900</td>\n",
              "      <td>0.555322</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Not an error, but Gemma3ForConditionalGeneration does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Fine-tuning complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a highly skilled translator specializing in English to Tamil translation. Your task is to:\n",
        "1. Accurately translate the provided English sentence into Tamil, preserving the original meaning, tone, and context.\n",
        "2. Provide only the translated Tamil sentence as output, without any additional text, explanations, or repeated translations.\n",
        "3. Ensure the translation is grammatically correct, natural, and culturally appropriate for Tamil speakers.\n",
        "4. Do not include any English text or extra phrases in the output.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def generate_translation(model, tokenizer, english_text, system_prompt):\n",
        "    \"\"\"\n",
        "    Generate Tamil translation for a given English sentence using the fine-tuned model.\n",
        "    \"\"\"\n",
        "\n",
        "    user_content = f\"{system_prompt}\\n\\nEnglish: {english_text}\"\n",
        "    messages = [{\"role\": \"user\", \"content\": user_content}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        use_cache=True,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    generated_ids = outputs[0][len(inputs[\"input_ids\"][0]):]\n",
        "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "model = FastLanguageModel.for_inference(model)  # Optimize for faster generation\n",
        "model.eval()  # Set model to evaluation mode\n",
        "model.to(\"cuda\")  # Ensure model is on GPU\n",
        "\n",
        "# --- Interactive Translation Loop ---\n",
        "print(\"--- Interactive English to Tamil Translator ---\")\n",
        "print(\"Enter an English sentence to translate into Tamil (or type 'quit' to exit).\")\n",
        "while True:\n",
        "    english_input = input(\"English sentence: \")\n",
        "    if english_input.lower() == 'quit':\n",
        "        break\n",
        "    if not english_input.strip():\n",
        "        print(\"Please enter a valid sentence.\")\n",
        "        continue\n",
        "    translation = generate_translation(model, tokenizer, english_input, SYSTEM_PROMPT)\n",
        "    print(f\"Tamil translation: {translation}\\n\")\n",
        "print(\"✅ Translator closed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_8jaA-pslht",
        "outputId": "0bdb67d8-1639-4a29-b35a-addad511bd02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Interactive English to Tamil Translator ---\n",
            "Enter an English sentence to translate into Tamil (or type 'quit' to exit).\n",
            "English sentence: I want to visit a historical place in Tamil Nadu.\n",
            "Tamil translation: நான் தமிழ்நாட்டில் ஒரு வரலாற்று இடத்தைப் பார்வையிட விரும்புகிறேன்.\n",
            "\n",
            "English sentence: The old lighthouse keeper, whose face was weathered by a lifetime of sea salt and relentless wind, watched with a quiet, knowing gaze as the tempestuous storm, which had been brewing on the horizon for days, finally unleashed its full fury upon the jagged coastline, even though he knew his sturdy, stone-built tower would withstand the punishing waves and the furious, driving rain.\n",
            "Tamil translation: கடல் உப்பு மற்றும் தொடர்ச்சியான காற்று மற்றும் ஒரு வாழ்நாள் முழுவதும் வழுவழுப்பான முகத்துடன், ஒரு அமைதியான, அறிவார்ந்த பார்வையைப் பயன்படுத்தி, பல நாட்களாக அடிவாரத்தில் கொந்தளித்த ஒரு கொந்தளிப்பான புயல் இறுதியாக வழுவழுப்பான கடற்கரையில் அதன் முழு கொந்தளிப்பையும் unleashed செய்தபோது, ​​அவர் தனது உறுதியான, கல் கட்டப்பட்ட கோபுரம் கடுமையான அலைகள் மற்றும் கொடிய, உதைக்கும் மழைக்கு தாக்குதலுக்கு உள்ளாகும் என்று знал.\n",
            "\n",
            "English sentence: Although the experimental procedure was meticulously followed by the research team, which had been working tirelessly for several weeks to replicate the groundbreaking results, the unexpected discrepancy in the data, which was subsequently traced back to a faulty sensor, rendered the entire set of findings unreliable and prompted a thorough reassessment of their methodology.\n",
            "Tamil translation: ஆய்வு குழுவின் ஆராய்ச்சி குழு, பல வாரங்களாகத் பிரறம்போதுமான முடிவுகளைப் பிரதிபலிக்கும் முயற்சியில் அயராது வேலை செய்திருந்தாலும், ஒரு தவறான சென்சார், தரவுத் தொகுப்பில் எதிர்பாராத முரண்பாட்டை ஏற்படுத்தியது, இது முழுத் தொகுப்பு முடிவுகளையும் நம்பமுடியாததாக்கியது மற்றும் அவர்களின் முறையீட்டை முழுமையாக மறு மதிப்பீடு செய்யத் தூண்டியது.\n",
            "\n",
            "English sentence: To test your fine-tuned Gemma model with more complex and longer English sentences for English-to-Tamil translation, I'll provide a new cell that includes 10 additional test sentences (a mix of short, medium, and longer sentences to assess generalization). This cell will use the same interactive translation setup from the previous response, with the enhanced system prompt and generation parameters to ensure clean, single-line Tamil translations.\n",
            "Tamil translation: Fine-tuned Gemma model-ஐ ஆங்கிலத்திலிருந்து தமிழ் மொழிபெயர்ப்புக்கான சிக்கலான மற்றும் நீண்ட ஆங்கில வாக்கியங்களுடன், நான் ஒரு புதிய cell-ஐ வழங்கப் போகிறேன், இதில் 10 கூடுதல் சோதனை வாக்கியங்கள் உள்ளன (குறுகிய, நடுத்தர மற்றும் நீண்ட வாக்கியங்களின் கலவை, பொதுமைப்படுத்தலை மதிப்பிட). இந்த cell, முந்தைய பதிலில் இருந்து அதே interactive translation setup-ஐப் பயன்படுத்தும், மேம்படுத்தப்பட்ட system prompt மற்றும் generation parameters-ஐப் பயன்படுத்தி, சுத்தமான, single-line Tamil translations-களை உறுதிசெய்யும்.\n",
            "\n",
            "English sentence: quit\n",
            "✅ Translator closed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "from google.colab import auth\n",
        "from google.cloud import translate_v2 as translate\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# --- Google Translate Setup ---\n",
        "auth.authenticate_user()\n",
        "# IMPORTANT: Replace these with your GCP Project ID and the API Key you created\n",
        "GCP_PROJECT_ID = \"\"\n",
        "GOOGLE_API_KEY = \"\"\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
        "translate_client = translate.Client()\n",
        "\n",
        "# --- Base Gemma Model (Un-tuned) ---\n",
        "print(\"Loading base Gemma 3 model...\")\n",
        "base_gemma_model, base_gemma_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME, max_seq_length=MAX_SEQ_LENGTH, dtype=None, load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# --- Llama 3 Model ---\n",
        "print(\"\\nLoading Llama 3 model...\")\n",
        "llama_model, llama_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", max_seq_length=MAX_SEQ_LENGTH, dtype=None, load_in_4bit=True,\n",
        ")\n",
        "\n",
        "print(\"\\n✅ All baseline models are ready.\")"
      ],
      "metadata": {
        "id": "oP_iEalSugUP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489,
          "referenced_widgets": [
            "1f463158a358413ab32b97db9015643f",
            "39d43cb0f5014bc7bd8f61f6b51b5870",
            "e8360ed0528f4b5580274fbbb8d23cb1",
            "45b738b72b694e46b939b529cdadfbd9",
            "0c7afa776a8f48d4b0bab221ee5c1393",
            "7091bf0db25b4878a1b94d699f697c97",
            "471ede1f3f274b3cacd544b098fe6c67",
            "82d409db8dad40d587d2cf1628b4b302",
            "74aad949a78c4c109ba22776416eb17b",
            "30d6b63f01b14770b2ea597b9a096f17",
            "e62521ef70014f3689bfe8ed5bd7978e",
            "79134f387de940a38a728d2a60786986",
            "9505ed0e5f364bb394c2ff1e007d9d91",
            "cbd6cd36eacd4123b0537afcf9122a4a",
            "abc900ef0984461bac179b560e1fdb0d",
            "933a91db79d5478caea41b579be2d978",
            "e618b2a471564852b1c0fc9ecb11a689",
            "045086fb88fe4ab38fd4904fecc26b2c",
            "2352a8e0f60d438c926fbcdbb5f1ea75",
            "34ea91326df34f7f8662efa0adc75795",
            "def4c8bc950d47adbd981291a556a2f4",
            "a9984e34a7c94a3a87d5e2044346d975",
            "27989f684b864f3a9ef01ac4d7ad1baa",
            "6fce3a886c234e39aa4fede3dc763812",
            "ee1cb3d483154ae1b3534c87eb3ae5d3",
            "6e3213ea62bf47eeb0a00a9645afc494",
            "159712403a1e44fc8ae3fac8c5180fe1",
            "a1319e20a2324aaca67a152fa3d5533d",
            "31a3d710dc394584bffe069855957037",
            "71979dd6238246e4b52f1a219ffe5b86",
            "4d5c807e3b9b4a68915669f071dbc688",
            "7111090ca5fd47c8b51a7cc9cb25bf7d",
            "4149122771694d3b9e9bb2a1c7406d30",
            "54d1ebe9458c420daaf8da019c7ed159",
            "3980a8064af04f0dbf3f520b774900f5",
            "716cb3f080d2443284013d60853d89c7",
            "f655a8e0e9d04a5ea67a5b8f5bee8fe2",
            "dc7ee2135d5d45328eb3141f23a7a695",
            "5e330e3b351443fb91d973fbafaf3809",
            "28fd00bff38540c395b9002c9e45d6c4",
            "fb1199858f43489eaed2ce01656eabea",
            "c2cf6e4b6570450689a013f74140baf9",
            "dd8bb51d02584a4a8d35dd0e7849a787",
            "d3b338f514f34a028134ba1876dda0a4",
            "45237464b5444e5692e400b00d9dd4be",
            "6a450f1637274d3683a627d111d51268",
            "b77a5e37a10f42f49ed9ccbf22a70c78",
            "720aa0c29562462db28d7d8abdcb221a",
            "59467fe329a24f6b9e6e85e9d4f5295a",
            "83542f31d85a43869ac9ad9f4ae2d1c5",
            "f135574a647b4a60958fb4c17eba06f0",
            "eeceae7b0dc44e5caa31c0e3c057de2d",
            "60fd7f8587a24fdb96ef98fb4ed15bbe",
            "3ead6fa9c81049b5b4f261fbc011efb1",
            "c51e143e826d4c5c927ee57474574023"
          ]
        },
        "outputId": "497dfb5e-87c3-412f-d101-dbf07ab2b2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base Gemma 3 model...\n",
            "==((====))==  Unsloth 2025.9.7: Fast Gemma3 patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
            "\n",
            "Loading Llama 3 model...\n",
            "==((====))==  Unsloth 2025.9.7: Fast Llama patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f463158a358413ab32b97db9015643f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79134f387de940a38a728d2a60786986"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27989f684b864f3a9ef01ac4d7ad1baa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54d1ebe9458c420daaf8da019c7ed159"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45237464b5444e5692e400b00d9dd4be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ All baseline models are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHASGEplBv8P",
        "outputId": "566cf2be-b5e5-44a4-d7cb-bb4f9ef0db57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.12/dist-packages (2.5.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_translation(model, tokenizer, english_text, system_prompt):\n",
        "    \"\"\"\n",
        "    Generate Tamil translation for a given English sentence using the fine-tuned model.\n",
        "    Handles both Gemma and Llama chat templates.\n",
        "    \"\"\"\n",
        "    if \"gemma\" in model.config.model_type:\n",
        "        user_content = f\"{system_prompt}\\n\\nEnglish: {english_text}\"\n",
        "        messages = [{\"role\": \"user\", \"content\": user_content}]\n",
        "    else:  # Assume Llama 3\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": english_text},\n",
        "        ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        use_cache=True,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "    generated_ids = outputs[0][len(inputs[\"input_ids\"][0]):]\n",
        "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "# Test the helper function with a sample input\n",
        "try:\n",
        "    test_text = \"Hello, everyone!\"\n",
        "    test_prompt = \"You are a highly skilled translator. Translate the following English text to Tamil accurately.\"\n",
        "    test_translation = generate_translation(model, tokenizer, test_text, test_prompt)\n",
        "    print(f\"Test translation: {test_translation}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error in generate_translation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JExI4q_j3vll",
        "outputId": "adba0e9a-c064-45a6-93bb-6b2fc9df4b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test translation: வணக்கம், எல்லோருக்கும்!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "references = []\n",
        "finetuned_gemma_translations = []\n",
        "base_gemma_translations = []\n",
        "llama3_translations = []\n",
        "\n",
        "print(\"\\n--- Starting evaluation loop ---\")\n",
        "for item in tqdm(eval_dataset, desc=\"Evaluating\"):\n",
        "    try:\n",
        "        # Extract English and reference Tamil texts\n",
        "        user_content = item['messages'][0]['content']\n",
        "        english_text_start_index = user_content.find(\"\\n\\n\") + 2\n",
        "        eng_text = user_content[english_text_start_index:].strip()\n",
        "        ref_text = item['messages'][1]['content']\n",
        "\n",
        "        # Store reference\n",
        "        references.append(ref_text)\n",
        "\n",
        "        # Generate translations\n",
        "        finetuned_trans = generate_translation(model, tokenizer, eng_text, SYSTEM_PROMPT)\n",
        "        base_gemma_trans = generate_translation(base_gemma_model, base_gemma_tokenizer, eng_text, SYSTEM_PROMPT)\n",
        "        llama3_trans = generate_translation(llama_model, llama_tokenizer, eng_text, SYSTEM_PROMPT)\n",
        "\n",
        "        # Store translations\n",
        "        finetuned_gemma_translations.append(finetuned_trans)\n",
        "        base_gemma_translations.append(base_gemma_trans)\n",
        "        llama3_translations.append(llama3_trans)\n",
        "\n",
        "        # Print sample for verification\n",
        "        if len(references) <= 3:  # Print first 3 for debugging\n",
        "            print(f\"\\nSample {len(references)}:\")\n",
        "            print(f\"English: {eng_text}\")\n",
        "            print(f\"Reference: {ref_text}\")\n",
        "            print(f\"Fine-tuned Gemma: {finetuned_trans}\")\n",
        "            print(f\"Base Gemma: {base_gemma_trans}\")\n",
        "            print(f\"Llama 3: {llama3_trans}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing item {item}: {e}\")\n",
        "        references.append(\"\")\n",
        "        finetuned_gemma_translations.append(\"\")\n",
        "        base_gemma_translations.append(\"\")\n",
        "        llama3_translations.append(\"\")\n",
        "\n",
        "print(f\"\\nGenerated {len(references)} translations.\")\n",
        "print(f\"Sample reference: {references[0] if references else 'None'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AGh-V0UCqax",
        "outputId": "86161358-104a-465b-a4ad-9cdd02f7fb51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting evaluation loop ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:   0%|          | 1/204 [00:20<1:08:30, 20.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample 1:\n",
            "English: The project will have been completed.\n",
            "Reference: திட்டம் முடிக்கப்பட்டிருக்கும்.\n",
            "Fine-tuned Gemma: திட்டம் முடிந்துவிட்டது.\n",
            "Base Gemma: திட்டம் முடிந்துவிட்டது.\n",
            "Llama 3: திட்டம் முடிக்கப்பட்டுவிடும்.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   1%|          | 2/204 [00:32<51:45, 15.37s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample 2:\n",
            "English: You should practice a little every day.\n",
            "Reference:  நீங்கள் ஒவ்வொரு நாளும் கொஞ்சம் பயிற்சி செய்ய வேண்டும்.\n",
            "Fine-tuned Gemma: நீங்கள் ஒவ்வொரு நாளும் ஒரு சிறிய அளவாவது பயிற்சி செய்ய வேண்டும்.\n",
            "Base Gemma: ஒவ்வொரு நாளும் கொஞ்சம் பயிற்சி செய்யுங்கள்.\n",
            "Llama 3: நாட்டில் ஒரு நாள் பயிற்சி செய்யுங்கள்.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   1%|▏         | 3/204 [00:41<42:16, 12.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample 3:\n",
            "English: Festivals in the village are very traditional.\n",
            "Reference:  கிராமத்தில் பண்டிகைகள் மிகவும் பாரம்பரியமாக இருக்கும்.\n",
            "Fine-tuned Gemma: கிராமத்தில் உள்ள திருவிழாக்கள் மிகவும் பாரம்பரியமானவை.\n",
            "Base Gemma: கிராமத்து பண்டிகங்கள் மிகவும் பாரம்பரியமானவை.\n",
            "Llama 3: கிராமத்தில் விழாக்கள் வரலாற்றுப்படி ஆகும்.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 204/204 [31:34<00:00,  9.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated 204 translations.\n",
            "Sample reference: திட்டம் முடிக்கப்பட்டிருக்கும்.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating BLEU and chrF++ scores\n",
        "print(\"\\n--- Calculating BLEU and chrF++ scores ---\")\n",
        "try:\n",
        "    chrf = sacrebleu.metrics.CHRF(word_order=2)  # chrF++ metric\n",
        "\n",
        "    # Fine-tuned Model Scores\n",
        "    bleu_finetuned = sacrebleu.corpus_bleu(finetuned_gemma_translations, [references])\n",
        "    chrf_finetuned = chrf.corpus_score(finetuned_gemma_translations, [references])\n",
        "\n",
        "    # Base Gemma Scores\n",
        "    bleu_base_gemma = sacrebleu.corpus_bleu(base_gemma_translations, [references])\n",
        "    chrf_base_gemma = chrf.corpus_score(base_gemma_translations, [references])\n",
        "\n",
        "    # Llama 3 Scores\n",
        "    bleu_llama3 = sacrebleu.corpus_bleu(llama3_translations, [references])\n",
        "    chrf_llama3 = chrf.corpus_score(llama3_translations, [references])\n",
        "\n",
        "    # Display Results\n",
        "    results = {\n",
        "        \"Model\": [\"Your Fine-tuned Gemma\", \"Base Gemma (Untuned)\", \"Llama 3 8B Instruct\"],\n",
        "        \"BLEU Score\": [f\"{bleu_finetuned.score:.2f}\", f\"{bleu_base_gemma.score:.2f}\", f\"{bleu_llama3.score:.2f}\"],\n",
        "        \"chrF++ Score\": [f\"{chrf_finetuned.score:.2f}\", f\"{chrf_base_gemma.score:.2f}\", f\"{chrf_llama3.score:.2f}\"]\n",
        "    }\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(\"\\n--- Evaluation Results ---\")\n",
        "    print(results_df.to_string(index=False))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error calculating metrics: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDojAJpHLlW_",
        "outputId": "23fe0faa-fd40-4a1f-96c5-af9fac8900ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Calculating BLEU and chrF++ scores ---\n",
            "\n",
            "--- Evaluation Results ---\n",
            "                Model BLEU Score chrF++ Score\n",
            "Your Fine-tuned Gemma      36.12        65.25\n",
            " Base Gemma (Untuned)      28.84        59.95\n",
            "  Llama 3 8B Instruct       2.94        21.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# --- 1. Save LoRA adapters ---\n",
        "try:\n",
        "    model.save_pretrained(\"gemma_tamil_translator_lora\")\n",
        "    tokenizer.save_pretrained(\"gemma_tamil_translator_lora\")\n",
        "    print(\"✅ LoRA adapters saved to 'gemma_tamil_translator_lora'\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Error saving LoRA adapters: {str(e)}\")\n",
        "\n",
        "# --- 2. Merge adapters and save in full PyTorch format ---\n",
        "try:\n",
        "    merged_model = model.merge_and_unload()\n",
        "    merged_model.save_pretrained(\"gemma_tamil_translator_full_pytorch\", safe_serialization=True)\n",
        "    tokenizer.save_pretrained(\"gemma_tamil_translator_full_pytorch\")\n",
        "    print(\"\\n✅ Full PyTorch model saved to 'gemma_tamil_translator_full_pytorch' (in .safetensors format)\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Error merging and saving PyTorch model: {str(e)}\")\n",
        "\n",
        "# --- 2a. Save merged model as a single .pt file ---\n",
        "try:\n",
        "    torch.save(merged_model.state_dict(), \"gemma_tamil_translator.pt\")\n",
        "    print(\"\\n✅ Merged model weights saved as single 'gemma_tamil_translator.pt' file\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Error saving .pt file: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkT1SZfW0E6O",
        "outputId": "dcdfec99-7721-425d-ae06-3682aa5afecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LoRA adapters saved to 'gemma_tamil_translator_lora'\n",
            "\n",
            "✅ Full PyTorch model saved to 'gemma_tamil_translator_full_pytorch' (in .safetensors format)\n",
            "\n",
            "✅ Merged model weights saved as single 'gemma_tamil_translator.pt' file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cii15Zrg3UiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524,
          "referenced_widgets": [
            "f7c6eec4e85d471483712c28de3ef69a",
            "4577b05504c742289bc64c5408c6e513",
            "10478204b949424aa15f99d33920836f",
            "54e90386e58745f1b5ca78c822aea39b",
            "4136013bba0b414bbb1d72fd31e589b5",
            "f2c065bd3d0c47e78e1068ed22dfba96",
            "dc95954c21ef48dc85bd0f940ed98412",
            "3ca0f47aef184a5b8d245d0036ebfe49",
            "4fe17aaa482a47d892315fec6170af70",
            "f615f2aa860644f98eaff8493ea5ba0e",
            "f163171a5d8d4ef4a3aeaea6d9c93e09",
            "ca571b92adc74b30933f7f98fc82f354",
            "643aa5d4e49b4e89acff57f620f8cf89",
            "79e06cb7c18049a790a1bce93097e692",
            "69f7e23da87f4952a8f2c8c90059bbbe",
            "aa7571ac897d47bb9883d1628e8b969d",
            "5854815d983043b0b17c9ee2a71bf3e3",
            "43199261030341e1a93707f814db135b",
            "a9c1b02a8bff48e49fb43694c65beb32",
            "8602b8be7dc445a18ad71bbf3f55bb90",
            "0f7b4c9494d04041a58e9d16989424a1",
            "f3af38b66bb94f8b8f0b70d653b873af",
            "90678b08b2ed496088e87a5e3ab548b0",
            "2f36575a933b4fb88327502dafaeff2a",
            "76dfe08657e04b7882418d354bad650f",
            "81fc7b7dffe24463b13d16908573a761",
            "f2f2f723ef1149f6813db655a7384282",
            "d263a3a2a32d4f5a99ecdc16f88e09c3",
            "00f0746cf3374f7daaab5ffa76052e17",
            "c269e5014c8e450cb97fca6432e80496",
            "d8b79d3129014a079423ae040ace267f",
            "a98d0949401847e3a85d1ac43670346f",
            "7a0d6ba7135d4cfa8a70d24331e85fa7",
            "ee023430ea1043d99abdcb3a7b08b2af",
            "c2bb4958d11e4dcfad723635f63f200d",
            "c8cc923ab02f4d3eaa0eeb8a4bb35054",
            "cb21cf99e9a249e2aaa1dd3833017928",
            "1d83b6540e7b488a98c713cfa2e23e37",
            "ce9664639aa344fe8f0bf84188ec503e",
            "6eff8a5bc68640f6818bb985b5535b31",
            "a9b6fc958a9d4a0faff45686f4ab464b",
            "a772649a210b49bf9bb9e3baf91184db",
            "f2b1cb2b94844d1d8fa6f73b104ee1b0",
            "6b6cc8672bfd42219d81f21ecbc261e3",
            "f73c028525bd4d5d8caec8c8c9886283",
            "045a79b26be64976b5b4aa85dcd5fe11",
            "565170ea922847639aa0ed49403e0ab0",
            "cfc0232a1fdb49429bfebf04287693b9",
            "7c5a4658740548a0890fa1390427fcb0",
            "f5d4ff15d4cb4c25b01fd45d36ae418a",
            "96478a898e3e4abdb6e1814dfa7231fa",
            "32e77158da00477c880f05fc06e3476d",
            "a0a68454d63d40fba94f6b4a9ce565ce",
            "5f8fea01a1ee46e3a23cf3bf606c4216",
            "424e520788944308af5c10636f2be2f8"
          ]
        },
        "id": "0GRzvXuc2gTI",
        "outputId": "cb0b46bb-2e92-463c-97f3-e3cf9f73773f"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from huggingface_hub import HfApi, upload_folder\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# --- Install required packages ---\n",
        "try:\n",
        "    !pip install huggingface_hub\n",
        "    print(\"✅ Installed huggingface_hub package\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Error installing huggingface_hub: {str(e)}\")\n",
        "\n",
        "# --- Get Hugging Face token ---\n",
        "try:\n",
        "    # Assuming HF_TOKEN is set in Colab secrets\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "    if not HF_TOKEN:\n",
        "        raise ValueError(\"HF_TOKEN not found in Colab secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Error accessing HF_TOKEN: {str(e)}\")\n",
        "    print(\"Set HF_TOKEN in Colab secrets (key icon > Add new secret > Name: HF_TOKEN, Value: your_token)\")\n",
        "    raise\n",
        "\n",
        "# --- Define repository name ---\n",
        "REPO_ID = \"arsath-sm/gemma3-tamil-translator\"\n",
        "LOCAL_MODEL_PATH = \"gemma_tamil_translator_full_pytorch\" # Path to the saved full model\n",
        "\n",
        "# --- Push saved model files from disk to Hugging Face Hub ---\n",
        "print(f\"\\n--- Pushing model files from '{LOCAL_MODEL_PATH}' to Hugging Face Hub at '{REPO_ID}' ---\")\n",
        "try:\n",
        "    api = HfApi()\n",
        "\n",
        "    # Create the repository if it doesn't exist (optional, but good practice)\n",
        "    # You might need to handle potential errors if the repo already exists\n",
        "    try:\n",
        "        api.create_repo(repo_id=REPO_ID, token=HF_TOKEN, repo_type=\"model\")\n",
        "        print(f\"✅ Created repository: {REPO_ID}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ℹ️ Repository {REPO_ID} might already exist or there was an error creating it: {e}\")\n",
        "        # Continue assuming the repo exists\n",
        "\n",
        "    # Upload the entire folder containing the model files\n",
        "    upload_folder(\n",
        "        folder_path=LOCAL_MODEL_PATH,\n",
        "        repo_id=REPO_ID,\n",
        "        repo_type=\"model\",\n",
        "        token=HF_TOKEN,\n",
        "        commit_message=f\"Upload {REPO_ID} model files\"\n",
        "    )\n",
        "\n",
        "    print(f\"\\n✅ Model files successfully pushed to Hugging Face Hub at {REPO_ID}\")\n",
        "    print(\"You can now use it via the free Inference API or load it with its name.\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Error pushing to Hugging Face Hub: {str(e)}\")\n",
        "    print(\"Check HF_TOKEN, REPO_ID, or network connectivity. Ensure the repository doesn't already exist.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.8.3)\n",
            "✅ Installed huggingface_hub package\n",
            "\n",
            "--- Pushing model files from 'gemma_tamil_translator_full_pytorch' to Hugging Face Hub at 'arsath-sm/gemma3-tamil-translator' ---\n",
            "✅ Created repository: arsath-sm/gemma3-tamil-translator\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7c6eec4e85d471483712c28de3ef69a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca571b92adc74b30933f7f98fc82f354"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...l_pytorch/tokenizer.model: 100%|##########| 4.69MB / 4.69MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90678b08b2ed496088e87a5e3ab548b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...ll_pytorch/tokenizer.json:  50%|#####     | 16.7MB / 33.4MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee023430ea1043d99abdcb3a7b08b2af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...pytorch/model.safetensors:   0%|          | 5.37MB / 3.23GB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f73c028525bd4d5d8caec8c8c9886283"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Model files successfully pushed to Hugging Face Hub at arsath-sm/gemma3-tamil-translator\n",
            "You can now use it via the free Inference API or load it with its name.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "\n",
        "# --- Configuration ---\n",
        "# List of files and directories to include in the zip\n",
        "FILES_TO_ZIP = [\n",
        "    \"gemma_tamil_translator_lora\",           # LoRA adapters directory\n",
        "    \"gemma_tamil_translator_full_pytorch\",   # Full PyTorch model directory\n",
        "    \"gemma_tamil_translator.pt\",             # Single .pt file\n",
        "    \"unsloth_compiled_cache\",\n",
        "    \"wandb\"\n",
        "    \"outputs\"                               # Training outputs (checkpoints, logs)\n",
        "]\n",
        "\n",
        "# Name the zip file with a timestamp for uniqueness\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "ZIP_FILE = f\"gemma_tamil_translator_files_{timestamp}.zip\"\n",
        "\n",
        "# --- Create the zip file ---\n",
        "print(f\"\\n--- Creating zip file: {ZIP_FILE} ---\")\n",
        "with zipfile.ZipFile(ZIP_FILE, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for item in FILES_TO_ZIP:\n",
        "        if os.path.exists(item):\n",
        "            if os.path.isfile(item):\n",
        "                # If it's a file, add it directly\n",
        "                print(f\"Adding file: {item}\")\n",
        "                zipf.write(item, os.path.join(\"gemma_tamil_translator_files\", item))\n",
        "            elif os.path.isdir(item):\n",
        "                # If it's a directory, walk through and add all files\n",
        "                print(f\"Adding directory: {item}\")\n",
        "                for root, _, files_in_dir in os.walk(item):\n",
        "                    for file_name in files_in_dir:\n",
        "                        file_path = os.path.join(root, file_name)\n",
        "                        # Store with relative path inside zip\n",
        "                        arcname = os.path.join(\"gemma_tamil_translator_files\", root, file_name)\n",
        "                        print(f\"  Adding: {file_path}\")\n",
        "                        zipf.write(file_path, arcname)\n",
        "        else:\n",
        "            print(f\"⚠️ Item not found: {item}\")\n",
        "\n",
        "print(f\"\\n✅ Zip file created: {ZIP_FILE}\")\n",
        "\n",
        "# --- Download the zip file ---\n",
        "print(\"\\n--- Initiating download ---\")\n",
        "files.download(ZIP_FILE)\n",
        "print(\"✅ Download initiated. Check your browser's download folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6j5dmF-_8VSA",
        "outputId": "a2bf0faa-10ea-46f9-f32a-2d0967af0586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Creating zip file: gemma_tamil_translator_files_20250924_091234.zip ---\n",
            "Adding directory: gemma_tamil_translator_lora\n",
            "  Adding: gemma_tamil_translator_lora/processor_config.json\n",
            "  Adding: gemma_tamil_translator_lora/added_tokens.json\n",
            "  Adding: gemma_tamil_translator_lora/tokenizer.json\n",
            "  Adding: gemma_tamil_translator_lora/tokenizer_config.json\n",
            "  Adding: gemma_tamil_translator_lora/chat_template.jinja\n",
            "  Adding: gemma_tamil_translator_lora/special_tokens_map.json\n",
            "  Adding: gemma_tamil_translator_lora/tokenizer.model\n",
            "  Adding: gemma_tamil_translator_lora/README.md\n",
            "  Adding: gemma_tamil_translator_lora/preprocessor_config.json\n",
            "  Adding: gemma_tamil_translator_lora/adapter_config.json\n",
            "  Adding: gemma_tamil_translator_lora/adapter_model.safetensors\n",
            "Adding directory: gemma_tamil_translator_full_pytorch\n",
            "  Adding: gemma_tamil_translator_full_pytorch/processor_config.json\n",
            "  Adding: gemma_tamil_translator_full_pytorch/added_tokens.json\n",
            "  Adding: gemma_tamil_translator_full_pytorch/tokenizer.json\n",
            "  Adding: gemma_tamil_translator_full_pytorch/tokenizer_config.json\n",
            "  Adding: gemma_tamil_translator_full_pytorch/chat_template.jinja\n",
            "  Adding: gemma_tamil_translator_full_pytorch/model.safetensors\n",
            "  Adding: gemma_tamil_translator_full_pytorch/special_tokens_map.json\n",
            "  Adding: gemma_tamil_translator_full_pytorch/generation_config.json\n",
            "  Adding: gemma_tamil_translator_full_pytorch/tokenizer.model\n",
            "  Adding: gemma_tamil_translator_full_pytorch/preprocessor_config.json\n",
            "  Adding: gemma_tamil_translator_full_pytorch/config.json\n",
            "Adding file: gemma_tamil_translator.pt\n",
            "Adding directory: unsloth_compiled_cache\n",
            "  Adding: unsloth_compiled_cache/unsloth_compiled_module_gemma3.py\n",
            "  Adding: unsloth_compiled_cache/UnslothPPOTrainer.py\n",
            "  Adding: unsloth_compiled_cache/UnslothORPOTrainer.py\n",
            "  Adding: unsloth_compiled_cache/ConvTranspose2d.py\n",
            "  Adding: unsloth_compiled_cache/Linear_peft_forward.py\n",
            "  Adding: unsloth_compiled_cache/Linear4bit_peft_forward.py\n",
            "  Adding: unsloth_compiled_cache/BatchNorm1d.py\n",
            "  Adding: unsloth_compiled_cache/AqlmLoraLinear_peft_forward.py\n",
            "  Adding: unsloth_compiled_cache/LoraParallelLinear_peft_forward.py\n",
            "  Adding: unsloth_compiled_cache/UnslothKTOTrainer.py\n",
            "  Adding: unsloth_compiled_cache/ConvTranspose1d.py\n",
            "  Adding: unsloth_compiled_cache/Conv1d.py\n",
            "  Adding: unsloth_compiled_cache/AwqLoraLinear_peft_forward.py\n",
            "  Adding: unsloth_compiled_cache/BatchNorm2d.py\n",
            "  Adding: unsloth_compiled_cache/GPTQLoraLinear_peft_forward.py\n",
            "  Adding: unsloth_compiled_cache/ConvTranspose3d.py\n",
            "  Adding: unsloth_compiled_cache/UnslothRewardTrainer.py\n",
            "  Adding: unsloth_compiled_cache/unsloth_compiled_module_siglip.py\n",
            "  Adding: unsloth_compiled_cache/BatchNorm3d.py\n",
            "  Adding: unsloth_compiled_cache/LayerNorm.py\n",
            "  Adding: unsloth_compiled_cache/Linear8bitLt_peft_forward.py\n",
            "  Adding: unsloth_compiled_cache/RMSNorm.py\n",
            "  Adding: unsloth_compiled_cache/Conv3d.py\n",
            "  Adding: unsloth_compiled_cache/UnslothDDPOTrainer.py\n",
            "  Adding: unsloth_compiled_cache/Conv2d.py\n",
            "  Adding: unsloth_compiled_cache/GroupNorm.py\n",
            "  Adding: unsloth_compiled_cache/UnslothCPOTrainer.py\n",
            "  Adding: unsloth_compiled_cache/__pycache__/ConvTranspose1d.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/UnslothKTOTrainer.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/GPTQLoraLinear_peft_forward.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/Linear8bitLt_peft_forward.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/Conv1d.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/Linear4bit_peft_forward.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/GroupNorm.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/unsloth_compiled_module_siglip.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/BatchNorm3d.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/UnslothORPOTrainer.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/UnslothPPOTrainer.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/AwqLoraLinear_peft_forward.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/RMSNorm.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/LoraParallelLinear_peft_forward.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/UnslothRewardTrainer.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/Linear_peft_forward.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/BatchNorm2d.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/BatchNorm1d.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/LayerNorm.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/Conv2d.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/UnslothCPOTrainer.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/unsloth_compiled_module_gemma3.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/UnslothDDPOTrainer.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/Conv3d.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/ConvTranspose3d.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/ConvTranspose2d.cpython-312.pyc\n",
            "  Adding: unsloth_compiled_cache/__pycache__/AqlmLoraLinear_peft_forward.cpython-312.pyc\n",
            "⚠️ Item not found: wandboutputs\n",
            "\n",
            "✅ Zip file created: gemma_tamil_translator_files_20250924_091234.zip\n",
            "\n",
            "--- Initiating download ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_43e6b1be-b7b5-42a5-bd86-70a8af04428f\", \"gemma_tamil_translator_files_20250924_091234.zip\", 5731660203)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Download initiated. Check your browser's download folder.\n"
          ]
        }
      ]
    }
  ]
}